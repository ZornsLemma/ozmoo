Working towards stable 7.x release, TODO:
- test on Electron
- finish reviewing existing SFTODO comments and dealing with them or promoting them to SFTODONOW
  - done acorn.asm
  - done acorn-cache.asm
  - done acorn-constants.asm
  - done acorn-disk.asm
  - done acorn-findswr.asm
  - done constants.asm
  - done memory.asm
  - done objecttable.asm
  - done ozmoo.asm
  - done screen.asm
  - done screenkernal.asm
  - done stack.asm
  - done streams.asm
- deal with SFTODONOW comments
- do regression test to check performance is comparable to 5.x/6.x semi-stable release
- testing! do some of my own, ask people on stardot (once I have something *I* don't intend to tinker with unless bugs are found)



Timings to try to reassure myself I've un-broken everything
"older" timings are just taken from the most recent values I can see below, these have not been recreated - I am looking for a sanity check more than anything, and I've doublede the older times to compensate for jiffies change
				new	older
B 144K SWR meddyn mode 7	$00924f $0092c6 OK
B 16K SWR meddyn mode 7		$014a10 $0123e4 older B+16K time before shadow support added - probably OK, the B+ would have had an extra 1K RAM
B 144K SWR+3MHz copro 		$005910 $0058b2 OK
B 16K SWR ext6502		$005a35 -       plausibly a little bit slower
M128 bigdyn			$009478 $009380 OK
B+128K bigdyn			$009373	$009336 OK
BeebEm Integra-B 64K mode 7	$009549	$009500 OK


M128 96K SWR mode 7 $0045d8 alpha 14-wip
ditto alpha 12 $004580 start $0003cc (that was at 500% but pretty much same at 100%)
alpha 12 ditto but with internal copro $3d5 to $0025d1

On C64 first_banked_memory_page=$d0, so it looks like the vmem cache stuff is used for 12K of RAM. Compare that with 0-19K of RAM on Acorn. I don't think this necessarily suggests I should add more cache compared to C64; my experiments (see below) with this suggest 5 cache pages was optimal, at least for HH. Just a note really.

Playing around with TRACE_SETJMP to see how low S can get: $ea

B 144K SWR meddyn $004963 fractionally slower than old 5.x, possibly teletext colour UI delay - not too worried (old 5.x was $004906)
All shadow versions are bigdyn now (so not "fair" to compare with meddyn, but OK for sanity checks etc)
B+ no SWR mode 7 $00592d
B+ no SWR mode 3 $00c0c2
B+ no SWR mode 0 $010b59
B+ 16K SWR mode 7 $0049b4 better than below, poss due to fixing the himem table glitch? anyway, it's fine
B+ 16K SWR mode 3 $0058bc
B+ 16K SWR mode 0 $005aff
B+ 32K SWR mode 7 $0048e5
B+ 32K SWR mode 3 $005422
B+ 32K SWR mode 0 $00544e
B+ 48K SWR mode 7 $0049a7
B+ 48K SWR mode 3 $0053a6
B+ 48K SWR mode 0 $0053a7
B+ 64K SWR mode 7 $00499b
B+ 64K SWR mode 3 $00547a
B+ 64K SWR mode 0 $005441
B+ 80K SWR mode 7 $0046a8
B+ 80K SWR mode 3 $0054dd
B+ 80K SWR mode 0 $005451
B+ 96K SWR mode 7 $0046a9
B+ 96K SWR mode 3 $0051c1
B+ 96K SWR mode 0 $005134
B+ 112K SWR mode 7 $0046a8
B+ 112K SWR mode 3 $0051bd
B+ 112K SWR mode 0 $005134
B+ 128K SWR mode 7 $0046a8
B+ 128K SWR mode 3 $0051bc
B+ 128K SWR mode 0 $005132
B+ 144K SWR mode 7 $0046ac
B+ 144K SWR mode 3 $0051be
B+ 144K SWR mode 0 $005135
M128 64K SWR mode 7 $0049c0
M128 64K SWR mode 3 $005624
M128 64K SWR mode 0 $00560f
These Integra-B timings done at fixed speed 100 on BeebEm and I'm guessing disc access is free given the complete lack of difference caused by different RAM sizes, but I'm mainly testing for correctness here.
Integra-B no SWR won't run
Integra-B 16K SWR mode 7 $004944
Integra-B 16K SWR mode 3 $005b2d
Integra-B 16K SWR mode 0 $005b2c
Integra-B 32K SWR mode 7 $00493a
Integra-B 32K SWR mode 3 $005ad1
Integra-B 32K SWR mode 0 $005ab2
Integra-B 48K SWR mode 7 $0049d7
Integra-B 48K SWR mode 3 $005b40
Integra-B 48K SWR mode 0 $005b22
Integra-B 64K SWR mode 7 $004a80
Integra-B 64K SWR mode 3 $005bf2
Integra-B 64K SWR mode 0 $005bca


Using the 12K private RAM:
B+ 16K SWR+shadow+12K private mode 7 $004b40 nice! ($005a51 below without private 12K)
Integra-B 64K $004a1f - slightly slower than before, but 64K is always a little iffy
Integra-B 16K $004324
Integra-B 16K disabling extra 12K 
Integra-B 0K SWR 12K private $005593
OK, these timings aren't making any sense, let's just start from scratch. I'll use total "SWR" as reported on Ozmoo loader screen.
Integra-B 12K $00559b
Integra-B 28K $004944
Integra-B 44K $0042fc - maybe this is b-em vs BeebEm (at 100x speed) but a M128 with 64K SWR takes $004901 so I don't really believe this
Integra-B 60K 
BeebEm M128 $004db7


b-em
                	with shadow		without shadow (rebuilt with --no-shadow-vmem)
B+ 64K SWR mode 0 	$0053de
B+ 64K SWR mode 3	$005402
B+ 64K SWR mode 6	$004e02
B+ 64K SWR mode 7	$00497d
B+ 16K SWR mode 0	$00bb49			$00bb22
B+ 16K SWR mode 3	$008bc1			$00bb1d
B+ 16K SWR mode 6	$0062a4			$00b4ba
B+ 16K SWR mode 7	$005a51			$00b002 *
* I forgot to remove the ADFS ROM before doing any of these tests; with it removed this goes down to $0091f0, which is consistent with earlier timings. I'll do all these with ADFS present, I'm sanity checking more than getting absolute performance numbers here.
- that looks good, just a whisker slower with the new code in mode 0 but I think very acceptable, in other modes the extra RAM really helps the 16K SWR machine, even an extra 4k from running in mode 3 helps.
B+ 64K SWR mode 7 shadow ADFS (should be similar given PAGE was &1f00 anyway) $004a09
B+ 16K SWR mode 7 shadow ADFS $005b83

BeebEm B-no-shadow 16K SWR Acorn DFS 1.2 $006d1a
ditto but MRM-E00 DFS $005b3c - so the extra RAM is probably helping a bit (and *is* being used)


"Proper" shadow RAM vmem support timings
M128 mode 7
0K SWR smalldyn 5 cache pages $006af8 (this didn't fit in prototype!?)
16K SWR meddyn 5 cache pages $004e39 - prototype took $004e2e but that's probably noise
32K SWR meddyn 5 cache pages $004976
48K SWR meddyn 5 cache pages $00499b
64K SWR meddyn 5 cache pages $0049f1

B+128K PAGE=&1F00 mode 7 (5 *recommended* cache pages)
16K SWR meddyn $00690e ($0091f2 with 16K SWR in 5.x pre-first-alpha timings below!)
64K SWR meddyn $004a37

B+128K PAGE=&1F00 mode 7 (5 *recommended* cache pages) using shadow copy code at &AF00
16K SWR meddyn $0053e7 (BeebEm gives $006371 for B+128K PAGE=&1900, FWIW)
64K SWR meddyn $0049d3

Integra B (BeebEm - performance is not comparable to b-em, probably due to disc access?)
16K SWR meddyn $0051cf
64K SWR meddyn $0049fd

Elkulator (self-reported)
MRB shadow mode 16K SWR meddyn $005cad (was $00524c in first 5.x public alpha) - so this is *slower*!?
new code 04:43:28-04:53:09=581 seconds
alpha-5.x branch no shadow support 04:54:17-05:07:18=781 seconds 
- so shadow support makes things 25% faster
- so I guess disc access time is probably "free" according to the self-reported time and thus highly misleading here



Notes from experiments with the two alternate spare shadow RAM strategies

cache=shadow RAM backed by dedicated cache (upstream)
swap=shadow RAM swapped with non-shadow RAM (mine)

All M128 mode 7 19K shadow RAM

cache tuning, 16K SWR:
meddyn 3 cache pages $004e8b
meddyn 4 cache pages $004e5d
meddyn 5 cache pages $004e2e <-- joint best meddyn
meddyn 6 cache pages $004e2e
smalldyn 3 cache pages $004c93
smalldyn 4 cache pages $004c5a
smalldyn 5 cache page $004c20 <-- best smalldyn
smalldyn 6 cache pages $004c48
bigdyn 3 cache pages $004e64
bigdyn 4 cache pages $004e29
bigdyn 5 cache pages $004de0 <-- best bigdyn
bigdyn 6 cache pages $004de4

(As "usual" - this is all a bit fuzzy - meddyn is maybe suboptimal for a machine with shadow RAM, although saving on bounce buffer copies does help with machines with very little RAM, e.g. a B+ with 16K SWR and not using any shadow. But maybe I should forget about meddyn here, where we have 16K SWR+19K shadow.)

cache tuning, 32K SWR: (not trying so many values here)
bigdyn 4 cache pages $00493a
bigdyn 5 cache pages $004924
meddyn 5 cache pages $00496e
smalldyn 5 cache pages $004797

cache tuning, 64K SWR: (not trying so many values here)
bigdyn 5 cache pages $004991
meddyn 5 cache pages $0049df
smalldyn 5 cache pages $00483c

cache tuning, *no* SWR:
smalldyn 3 cache pages $006a93
smalldyn 4 cache pages - won't fit (256 bytes short)




swap, *no* SWR:
smalldyn swap $0134ed (!) - profiling shows huge amounts of time spent on copy and swap loops, not too surprising

swap, 16K SWR:
smalldyn swap $004cca
meddyn swap $005060
bigdyn swap $004f27

swap, 32K SWR:
smalldyn swap $0047a3
meddyn swap $0049d6
bigdyn swap $004955

swap, 64K SWR:
smalldyn swap $004820
meddyn swap $0049eb
bigdyn swap $004988




nothing to do with shadow RAM, testing main 5.x branch now to see how "too large" memory (in the form of SWR, but don't think that's key) has an effect and if tweaking quick index size helps. M128, mode 7, smalldyn.
        qil=6 (default)              qil=7
32K SWR $004858
48K SWR $004702                      $00470a worse
64K SWR $004712 tiny bit slower!     $004711
80K SWR $0047a2 slower still!        $004793
96K SWR $004790 slower than 48/64K   $00477f

qil=6 again but with drive noises on:
48K SWR $004702
64K SWR $004713
I really thought drive noises would make a difference. Oh well. I still suspect that while searching the larger vmap slows things down a bit, on real hardware with slower drives the reduced access time will help. But it may be the extra accesses are pretty much one-offs anyway and so there's no real reduction in drive access in this optimised playthrough from the extra 16K of SWR going from 48->64.




Latest 5.x timings pre-alpha release:

More timings, all in b-em with drive noises off and using mode 7 bigdyn unless otherwise noted (bigdyn is the "default" for HH built in benchmark mode) - note that Elkulator *does* have drive noises on:

    					4.x		5.x
B 144K SWR+3MHz copro 			$002cc3		$002c59
B 144K SWR bigdyn			$004a22		$0047c8 3.2% faster
B 144K SWR meddyn					$004906 slower than bigdyn, but v big SWR
B 144K SWR+256K turbo copro		$002328		$00232e fractionally slower
M128 smalldyn				$00483f		$004714 1.6% faster
M128 bigdyn				$004c0e		$0048c4 4.3% faster
M128 bigdyn no SWR					$01ec79
M128 meddyn						$004901	5.x meddyn 0.3% worse than 5.x bigdyn
M128 meddyn ADFS					$0049d8
B+ 16K SWR bigdyn			$009852		$0093ec 2.9% faster
B+ 16K SWR meddyn					$0091f2 4.2% faster than 4.x bigdyn
B+ 32K SWR bigdyn					$004bc3
B+ 32K SWR meddyn					$004bf2 slightly worse than bigdyn
B 32K SWR 8271 bigdyn					$004c3e
B 32K SWR 8271 meddyn					$004af8 1.7% faster than 5.x bigdyn
Electron Elkulator 16K SWR		$00c025		
							$00b8a3	3.9% faster, iffy
					22:05:48-22:43:21=2255s
							18:14:23-18:51:12=2209s (2.0% faster)
Electron Elkulator 16K SWR+shadow	$005677		$00524c	4.8% faster, iffy
Electron Elkulator 16K SWR ADFS		$00fe96		$00ef5d
MAME Electron 64K SWR bigdyn		$00a535		$009f1e 3.7% faster, iffy
MAME Electron 64K SWR meddyn		        	$009ba8 2.2% faster than 5.x bigdyn

5.x Electron DFS build does have vmem_start at $4c00, so same as 4.x.

I do wonder if meddyn is only an advantage on systems which are swapping so badly that in practice they're mostly unusable and/or the extra slowdown caused by bigdyn would be imperceptible. I can't help feeling that meddyn should offer a small improvement over bigdyn in general, since it avoids the need for the complex screen hole skipping code - I guess that would only manifest on a B or Electron though. OK, having added B 32K SWR timings above, I am seeing meddyn offering an advantage over bigdyn on a system not starved for RAM. It is tempting to not use meddyn on shadow builds, but the B+ 16K SWR timings do show an improvement, although this is *probably* due to reduced copying via bounce buffer and may in real use be swamped by disc access times and be imperceptible.
- Am now also seeing an improvement on 64K electron for meddyn vs bigdyn, which would make sense since it has a screen hole



Electron benchmark vmem_start has now slipped back to $4c00; this is no worse than 4.x and I'm not planning to try to squeeze this back out right now unless the benchmark suffers.

M128 medium dyn $004912 - as on B 144K SWR, that is *slower* than bigdyn going by timings below - running now, bigdyn $0048cd, consistent-ish with time below
B 144K SWR medium $004968 - that is *slower* than bigdyn going by timings below - bigdyn $004a32 - I have no idea why bigdyn is slower than it used to be, but anyway it looks like medium is faster than bigdyn
- OK, that was with the dynmem adjust disabled - with that back on, medium dyn $004919 (not too surprising, adjust has little headroom in medium dyn), bigdyn $0047b3 - that's more like it
Elkulator 16K SWR medium $00b622 (note we have an extra 512 bytes of vmem cache compared to 4.x as well here)


OK, profiling a M128 masquerading as an Electron (non-shadow mode 6, PAGE=&E00, 16K SWR), ~2.8% of the runtime is taken up in the four instruction SWR bounce buffer copy loop. So what I think's happening is that because it's swapping so much (probably precisely as much as 4.x, but that's a lot), the CPU overhead of copying the data (which we read over and over again) into SWR is measurable, whereas in 4.x we were doing at least some of the reads into vmem cache in main RAM and avoiding that.

M128 16K SWR non-shadow mode 6 (using M128 as it's easy to have PAGE=&E00, for a "2MHz Electron" environment - though note there's no drive noises here unlike Elkulator) BUT THIS SEEMS TO BE USING SW SCROLLING!? $011e6d
OK, trying again with sw scroll - $0100b3 (repeat $0100af)

M128 8K SWR shadow mode 6 - so this is (if I didn't mess up) equivalent in terms of free RAM to the previous case, but we're not paying the cost of working around the screen hole $00f529

M128 16K SWR shadow mode 6 - mainly to see it's faster and "prove" I did restrict to 8K correctly before - $006fb3

extra before/after slow uses:
- B 144K SWR $0047b5 - not looking like a big loss
- Elkulator 16K SWR $00c3d5 - better than before, but still worse than 4.x :-( (1.9% slower) - story_start is now at $4c00 to "match" extra_vmem_start in 4.x, although I think we're still a handful of bytes larger which don't matter in this specific build due to alignment

ram_bank_list copied into zp:
- B 144K SWR $0047a5 - but I didn't this for the performance gains at this point, so not disappointed they're not huge

With sta_dynmem_ind_y_slow subroutine compacted:
- B 144K SWR $0047c0

With lda_dynmem_ind_y_slow subroutine compacted:
- B 144K SWR completes in $00479e - a bit *faster* than before!? but it's at the level of noise, main thing is it works (pure speculation, but even on a machine with more SWR than the game needs, freeing up some main RAM will probably be a small win, e.g. because of the "don't bother paging" code in main RAM, plus a little more data gets shifted to before the screen hole on a B-no-shadow so the slower cases in the bigdyn+screen hole code don't trigger as much

With WIP code compression via less macros:
- B 144K SWR completes in $00481a, which isn't too much slower than before (and this machine has so much SWR it won't get any benefit from extra memory free)
- M128 bigdyn $0048d9 - again, a bit worse, but not that much so
- both the above are 0.3% slower than 5.x used to be
- B+ 16K SWR $0093f6 - 0.1% slower than 5.x used to be
- I won't test Electron yet as it's slow, but the Electron build is 512 bytes better off than before this change, so we haven't clawed all the memory back yet, but we're definitely moving in the right direction. I might *guess* this extra 512 bytes is also present (it depends on alignment of course) on other builds, and may account for the reduced slowdown on a B+ 16K SWR compared to the other machines with more memory.


On 4.x, Electron ($e00 build) has extra_vmem_start at $4c00 in benchmark
On 5.x, story_start is $5000
So we're 1K worse off - which is likely to be significant. 5.x has 20K game RAM, 4.x has 21K game RAM - a 5% increase, and since we're probably struggling to keep the working set in memory that's probably disproprtionately significant. The fact I see comparable or slightly better performance for 5.x with "lots" of sideways RAM also suggests the code itself is not really slower on the Electron, though I haven't confirmed this.

On 5.x, program_end is at 0x4adb and stack_start is at 0x4c00 due to padding and alignment. So I think we'd need to save 731 bytes to claw back that 1K, although (I haven't checked) probably to be truly comparable we'd need to claw back some extra, as 4.x is *probably* not getting lucky and wasting no memory to alignment. Let's just check - it has 93 bytes of free space before stack_start, so a truly comparable 5.x would require saving 824 bytes.

egrep '[0-9a-f][0-9a-f][0-9a-f][0-9a-f].*(\+before_dynmem|\+after_dynmem|\+acorn_page)' temp-5.x/acme_report_ozmoo_electron_swr_e00|wc -l
- shows 70 expansions of the page in macros on Electron 5.x. Simplifying a bit, each one will be about 14 bytes so that's 980 bytes! Of course we can't possibly get rid of all these bytes, but if we replaced each with a jsr and ignore the small overhead of implementing those subroutines, we'd be saving 770 bytes.
- worth noting that if I do keep some/all of these as macros, we might save ~35 bytes by having a zp copy of ram_bank_list.
- the equivalent macros in 4.x are (assuming I didn't miss any; acorn_swr_page... and acorn_page...) invoked only six times - this will be the new "less often at runtime, more often in the code" bigdyn paging model


More timings, all in b-em with drive noises off and using mode 7 bigdyn unless otherwise noted (bigdyn is the "default" for HH built in benchmark mode) - note that Elkulator *does* have drive noises on:

    					4.x		5.x
B 144K SWR+3MHz copro 			$002cc3		$002c58 (0.9% faster)
B 144K SWR				$004a22		$0047e1 (3.0% faster)
B 144K SWR+256K turbo copro		$002328		$002334 (0.1% slower)
M128 smalldyn				$00483f		$004715 (1.6% faster)
M128 bigdyn				$004c0e		$004899 (4.5% faster)
B+ 16K SWR 				$009852		$0093c6 (3.0% faster)
Electron Elkulator 16K SWR		$00c025		$00c92f (!!!)
					22:05:48-22:43:21=2255s
							21:25:05-22:05:14=2409s damn! 6.8% slower
Electron Elkulator 16K SWR+shadow	$005677		$0056dd
- so it's a smidge worse on 16K+shadow Electron, but tolerably so
- the worrying thing is the straight 16K Electron performance - my *suspicion* is this is caused because the binary is that bit bigger so there's less vmem cache available, but I may be wrong

(Electron timings expressed in $xxxxxx format are as reported by Ozmoo; these are a bit inaccurate - presumably due to interrupts gettnig disabled during disc access - but I am assuming it makes some sort of sense to compare them.)



vmap_max_size tweak
- BBC B 144K SWR $0047e0 (before: $00481c)
- M128 $004897 (before: $004889)
- M128+3MHz copro $002ccc (before: $002cd9)
- OK, I don't think this is having a big effect (nor did I expect it to, it just seemed to at first)

vmap_z_l page-alignment waffling:
- currently we have hacked this to be at $701 on SWR builds - that page was wasted before, so this (with a proper implementation) is a win, as we will save ~254 bytes inside the binary (remember the initial vmap can be in discardable init code)
- tube build currently (except in non-default cases, like no host cache support, which is only an option for debugging really) has ~255 byte vmap_z_l inside the main binary, but it loads at $600. If we load at $700 but save that ~255 byte vmap_z_l we're no worse off. We'd only be worse off if vmap_max_size was typically under 255, but for tube+host cache or turbo tube it's not, and those are the primary cases.
- for simplicitly I think I could make $500 be the vmap_z_l buffer on all builds, relocating scratch (double) page to $600, with the tube build now loading at $700.

ZP requirements for my bigdyn support:
- lda_dynmem_ind_y_internal - 2 consecutive
- sta_dynmem_ind_y_internal - 3, of which 2 consecutive
- z_set_variable_reference_to_value - 3, of which 2 consecutive
- z_get_referenced_value - 3, of which 2 consecutive

Is HW scroll worth it in mode 7?
- BBC B 144K SWR sw scroll $004ac7
- BBC B 144K SWR hw scroll (with ugly colour code glitch) $0048e3
- so 2.6% slower with software scrolling
- remember mode 7 *benchmark* is already quite badly slowed down (in a not-human-perceptible way, I believe) by vsync stuff to avoid cursor on/off glitches, so there is precedent for minor slowdown in pursuit of decent appearance

Fixed probable bug which meant no use of HW scrolling (although it always defaults to SW scroll in mode 7 anyway):
- M128 $00489d (same, as expected)
- M128 mode 3 - initial time $0002ee - final $0055bb - so 423.94 seconds (cf 414 seconds with extra 64K in e-mail to Fredrik, although that was a smalldyn build and this is bigdyn) - with an extra 64K SWR final is $oops, game is completely broken!
- Electron (MAME, 64K SWR) 20:27:47-20:41:33, self-reported $009f6f (816.3 seconds), manual times give 826 seconds, so this is 3.8% faster than 3.7f
- Elkulator (shadow mode, 16K SWR) - $0058be

Timings with auto build (all bigdyn, mode 7):
- M128 $0048a0
- BBC B 144K SWR $004adb
- B+128K (PAGE=&1F00) $00485b ("should" be slower than M128, but maybe artefact of fast drive access!?)
- M128+3MHz copro $002cd3 (no drive access during play)
- M128+256K copro $0025aa (no drive access during play)
- Electron (MAME, 64K SWR) $00d1b6 - this seems slow, but I am not sure the self-timing can be trusted on Electron (but key thing right now is it completed OK) - repeat, screen cleared to start game play at 19:28:01, finished, 19:46:07 at final prompt -> 1086 seconds (the self-reported time $00d1cc similar to previous run and corresponds to 1074.16 seconds, so this is artificially low, especially as it also includes the initial load, but not *that* artificially low)
- Electron (Elkulator, 16K SWR) $00fd32 - not sure I trust this self-timing, but it worked

For reference:
3.7f MAME 64k SWR 20:05:25 - 20:19:44 (incidentally very visible flickering on status bar in MAME, as I'd expect) - reported at $00a598 - manual times give 859 seconds (compared to 847.84 seconds for the self-timing)

M128 mode 7 smalldyn "shadow only build" $004715 (repeat $00471d)
ditto but forcing use of "B no shadow capable build" $004733 so 0.16% slower (repeat $004731)
- this is about what I expected, as the screen hole code has a fairly small impact on smalldyn
- I am leaning towards only generating a single "BBC" executable for games which run in smalldyn on a B-no-shadow and other-BBC, since the penalty is so small
- *however* the "only" gain is a saving of ~12K on one executable, which frees up disc space for saves and/or a larger game file, but games running in smalldyn are likely to be fairly small anyway - still, albeit without compression, Tristam Island *probably* (not checked) runs in smalldyn but had to go onto a .dsd before I introduced compression (not entirely sure about this), so it maybe not worthless (and it is extra space for saves)
- OTOH, I may - probably not til after first public alpha, but still - be introducing support for using "spare" shadow memory. That will hopefully not complicate the code too much, but it will be extra code which is useless on the most memory constrained BBC, i.e. the B-no-shadow. So arguably it would be worth keeping separate no-shadow-BBC and shadow-BBC binaries anyway.
- OT3H, what about the Electron? I don't really want to have Electron-shadow and Electron-no-shadow binaries. Then again, I suppose nothing stops me adding the use-spare-shadow-memory code to the Electron build and the BBC-shadow without forcing it into the BBC-no-shadow build, so this is probably irrelevant.
- gut feeling is that *for now* I will avoid possibly-wasted work tweaking the build system to generate a single all-BBC executable now and then maybe reverse it later, I can always choose to go to a single-all-BBC executable later once I've done the use-spare-shadow-memory work and have a better view of the final list of tradeoffs.

M128 mode 7 smalldyn:
- 1K screen hole with $7c as immediate constant - $004747
- 1K screen hole with $7c held in zp - $00474b, so 0.02% slower (makes sense, smalldyn has nearly-free memory hole)
M128 mode 7 bigdyn:
- 1K screen hole with $7c as immediate constant - $004936
- 1K screen hole with $7c held in zp - $00494f, so 0.13% slower
I think I "have" to go with using zp, otherwise a) Electron build won't take advantage of shadow RAM b) a single B-no-shadow executable won't be able to support mode 6 or 7 c) as with Electron, might be desirable to have a single BBC executable which handles shadow and no shadow

M128 mode 7 bigdyn:
- screen hole support but should be disabled at runtime - $004960
- ditto but with acorn_screen_hole_start_page in zp - $00495a
- with no screen hole support - $00489b
BBC B mode 7 bigdyn - $004aea (used to be $004d51 with 3.7f, probably)




Latest code, M128 mode 7
- smalldyn $004776 (previous baseline was $004787, I think this is due to getting rid of a few bits of debug code, but very reasonable value)
- bigdyn $004900 (previous baseline was $004b71, this is a whisker faster than experimental bigdyn rework so that's reasonable)
Tweaking read_byte_at_z_address to reduce paging:
- smalldyn $004733 (0.36% saving, not bad for ~4 lines of code)
- bigdyn $0048ae (0.4% saving, not bad for ~4 lines of code)
Putting z_pc_mempointer_ram_bank in zero page:
- smalldyn $00472a (0.05% saving, worth having if I don't run out of zp but not mindblowing, code size saving may be useful) - got $00472d on a repeat
- bigdyn $00489d (0.09% saving, ditto) - worth noting *smalldyn* took $00483f in 3.7f!
Hackily arranging for vmap_z_l to be page-aligned+1:
- smalldyn $00472f - so within noise of before this change, *but* by chance the first ~188 vmap_z_l entries would incur no page-crossing penalty and that's enough for 96K read-only memory, so it makes sense that on this run it's not making any difference
So we are currently at:
- smalldyn mode 7 $00472f (drive noises on $004731; just possibly broken audio on the virt means this is having no effect)
- smalldyn mode 7 +64K SWR (no disc access during play) $0047cb WTF!? (repeat $0047c6)
- smalldyn mode 7 +64K SWR 16-entry quicklist $004764
- smalldyn mode 7 +64K SWR 12-entry quicklist $004745
- smalldyn mode 7 12-entry quicklist $004708!
- smalldyn mode 3 +64K SWR $0054de
- smalldyn mode 0 +64K SWR $0054e1
- initial time output at start of benchmark is $003cd (mode 7), $003da (mode 3), $003e0 (mode 0) - so all the same within noise limits I think
- so with 64K extra SWR (despite the superficial penalty; lack of disc access during play makes me fairly confident this would be reproducible on real hardware, whereas emulator disc access is artifically fast in general) we see "within benchmark" times of 414.86s (mode 0), 414.8s (mode 3), 347.86s (mode 7)

Experimental rework so bigdyn keeps Z-machine PC bank paged in by default:
- baseline is $004b71 for bigdyn, probably (not double-checked) $004787 for smalldyn
- initial hack $00494a - woohoo, 2.9% faster than old bigdyn, and "only" 2.5% slower than smalldyn
- omitting pha/pla in before_dynmen_read - $004901, so 3.2% faster than old bigdyn

FWIW 3.7 model B no shadow bigdyn takes $004d51 IIRC ($004c10 is prob 3.7f figure for M128, which has less SWR so is probably at a disadvantage, but it's not exactly apples-to-apples), so *maybe* (this may be unfair in all sorts of ways and is just a super quick hacky comparison) the mode-7-at-3c00 hack costs 1.6% in performance anyway, which if true would mean the 0.9% slowdown I am seeing for the 5.x screen hole support is probably still a net win. our_keyv is mostly free, but our_wrchv (to adjust the cursor position) is fairly hot, so that is plausible.

Distinct ZP addresses used by dynmem_ind macros:
- zp_temp
- z_low_global_vars_ptr
- z_high_global_vars_ptr
- object_tree_ptr (.zp_sibling, .zp_parent, .zp_dest)
- default_properties_ptr
- .zp_object (zp_mempos)
(To help with thinking about factoring code/partial code for macro into subroutines with hard-coded ZP addresses.)
This may be ignoring the doubly-wrapped uses in macro_string_array.../macro_parse_array..., so that adds string_array and parse_array to the above list

Current unoptimised bigdyn screenhole code takes $004e3b (and remember it's had 512 bytes of memory taken away, though that's not likely to make a huge difference in itself)
Non-screenhole bigdyn takes $004b71 (and I can reproduce this - actually $004b69 - with the "new" code with mem hole disable via conditional compilation - and hacking to not have a screenhole but write-off 2 pages of memory, to get a "fairer" comparison, I get $004b64 - there's obviously some kind of jitter in the timings here, perhaps due to 500% speed but not sure)
- so 3.7% slower. That's not great but it's not terrible, bearing in mind the complete lack of optimisation (there's paranoid code to preserve flags and some debugging checks and also some further optimisation potential).
- optimising local variable dynmem access takes us down to $004dd5 - so now 3.2% slower, not too bad for such a simple change
- WIP partial optimising/tidying - $004cce - so down to 1.8% slower
- more WIP - $004ce6 - so 1.9% slower
- more WIP - $004c5b - so 1.2% slower
- more WIP - $004c5e
- more WIP - $004c67 - so 1.3% slower
- more WIP - $004c46 - so 1.1% slower (bit illogical, random noise? maybe smaller code gives extra VM page?)
- more WIP - $004c2f
- more WIP - $004c22 - so 0.9% slower
- more WIP - $004c28 - illogical as would expect this to be faster
- more WIP - $004c22 - bit disappointing
- with screen hole support but with 0-size hole starting at $ff00 to disable it - $004c29 - bit illogical

Screen hole notes - transfer these into a permanent comment at top of acorn.asm (or somewhere) when/if this settles down:
- although this will be automatically true in practice, I think the screen hole needs to be a multiple of 512 bytes so as to avoid breaking alignment assumptions on read-only memory
- (bigdyn only) all dynmem accesses need to be checked, if the raw 6502 address calculated is >=start_of_screen_hole, it must be incremented by the corresponding number of pages
- the build system needs to take into account the potential screen hole when deciding how much RAM will be free for certain values of PAGE, similarly the loader probably needs to be tweaked in this regard
- SFTODO: HOW IS READ-ONLY ACCESS AFFECTED? I SUSPECT THERE ARE A FEW SUB-CASES
- for smalldyn, I suspect read-only access (via convert_index_x...) can accommodate the screen hole for (nearly) free, by just taking care to set vmem_blocks_in_main_ram correctly during initialisation and using (as ACORN_ELECTRON_SWR already does) screen_ram_start instead of flat_ramtop as the top of main RAM
- for bigdyn I suspect convert_index_x... can also handle a screen hole mostly for free, again just by ensuring vmem_blocks_in_main_ram and vmem_blocks_stolen_in_first_bank are set up correctly and using screen_ram_start as the top of main RAM



Why do I want to get rid of the screen hole?
- presumably any fix would work just as well with a mode 6 screen as a mode 7 screen
- this would mean a B-no-shadow could use mode 6 (user preference, accented characters) given sufficient SWR and small enough game dynmem
- it would also mean the Electron would be a lot less of a special case (it would mainly differ from B-no-shadow just in the Electron-style ROM paging)
- the macros to insert the screen hole are pretty annoying and intermittently cause Ozmoo to break when building some untried combinations of game+config
- it *might* (if the overhead is small/can be runtime patched) allow B-no-shadow and BBC-shadow to be combined into a single executable (not *overly* optimistic about this, but we'll see)
- it means the B-no-shadow executable can take advantage of arbitrary PAGE, instead of having to assume a fixed value (because the relocation code can't slide code round the screen hole) - this means the same game will work on DFS or (say) E00-MMFS and take advantage of the extra RAM where it's available

New upstream 5.3 with code reordered $004787 (repeat $00478e) - so 1.34% faster than previous, and 0.96% *faster* than 3.7f, yay!
- built experimentally with COMPLEX_MEMORY gives $004a88 - 4.2% slower than without, and 3.2% slower than 3.7f - and of course this is without actually altering read_byte_at_z_address to allow for a read hole
- experimental build with pseudo-COMPLEX_MEMORY only for global variable access - $004844 - so 1% slower than "standard" build (of couse still without modification to actually create a hole)

New upstream changes 5.3 (including some but not all of read_next_byte optimisations), M128 usual, smalldyn - $004887 (so 0.4% slower than 3.7f)

With latest upstream changes 5.3 ("usual" M128 no tube config, smalldyn) takes $004915, so 1.2% slower than 3.7f.

Looking at profiling for smalldyn benchmark 3.7f vs 5.3:
- is it possible read_byte_at_z_address is being called more now? Not too clear, but the lda (mempointer),y when it's the same 256 byte segment is hot in 5.3 and not in 3.7f
- read_next_byte (which calls read_byte_at_z_address unconditionally, and always has) is called way more often in 5.3 than 3.7f
- .read_new_byte inside read_byte_at_z_address is called *more* in 5.3, but not all that much (maybe twice as much, but it's not called all that much in the first place) - so it's the "same 256 byte segment" case which is potentially accounting for most of the lost performance
- read_byte_at_z_address is not called significantly more outside read_next_byte in 5.3 than 3.7f, so I think read_next_byte is a big part of the difference
- calc_address_in_byte_array used to handle dynmem specially and most of its invocations were dynmem, but I don't know if this in itself is responsible for much of the increased traffic via read_new_byte

M128, mode 7, 5.3 (fixed vmap), drive noises off, bigdyn - $004d3a (3.7f $004c10, so 1.5% slower)
M128, mode 7, 5.3 (fixed vmap), drive noises off, smalldyn - $004969 (3.7f $00483f, so 1.6% slower)
- so the slowdown on benchmark from 3.7f to current 5.3 is consistently about 1.5% with or without tube

M128+internal 6502, mode 7, 5.3 with probably broken initial vmap, drive noises off - $0027be
ditto but 3MHz external 6502 - $002f4a
ditto (3MHz) but with alpha 3.7f - $002d37 (and no disc access during play, unless I missed a brief one while watching it run and I probably didn't)
ditto but 5.3 with fixed make-acorn.py vmap handling - $002de8 (and no disc access during play) - so 1.5% slower than alpha 3.7f, which is mildly disappointing (particularly given the Z3 quick index optimisations due to the new vmap storage) but not terrible, and I haven't even tried to investigate this yet

Master 128, mode 7:
- before *FX19 use benchmark: $0049bd
- with *FX19 use: $004c0d
- so a 3.1% increase in runtime
- absolute increase is 592
- !if-ing out the osbyte to read vdu status gives $4c0f ($4c0d another time), i.e. negligible
- !if-ing out the VSYNC osbyte (I did see the solid block flicker in at one point) gives $49bf, i.e. back where it was before, as we'd expect
mode 3:
- before *FX19 $005850
- with $00592b
- so just under 1% increase in runtime
- absolute increase is 219
mode 6:
- before $0051b2
- with $0051f2
- 0.3% increase in runtime
- absolute increase is 64


Executable compression:
- OZMOOB load-and-decompress 1.59, load 1.29 (timed on M128)
- OZMOOSH l-a-d 1.74, load 1.42 (timed on M128)
- OZMOOE l-a-d 2.8, load 2.3 (timed with stopwatch using Elkulator in normal mode)
Without executable compression, timed in same way:
- OZMOOB load 1.81
- OZMOOSH load 1.74
- OZMOOE load 2.8
(OZMOO2P is awkward to time as it corrupts BASIC workspace)
So it looks like the compression doesn't hurt, since load-and-decompress times are pretty much the same as load times for uncompressed (and thus larger) files, and for OZMOOB (probably due to its screen hole) it's actually a small improvement. And of course given there's no time penalty compression is a win, since it frees up more space on the disc.


On b-em, a B with DFS 2.26 and drive noises on takes 3.02 seconds to load a 20K mode 2 splash screen.
lzsa1 compresses that down to 12.3K, which might cut the load time down to 1.86 seconds (spurious precision). If we can decompress 17600 bytes per second we will break even in terms of user experience. That's 352 bytes per frame. Obviously faster is better, but looking at https://github.com/emmanuel-marty/lzsa (and bearing in mind I don't know platform the graph is from - a spectrum?), that suggests exomiser is a bit slower than ideal (but the faster variants would probably just about break even) - that fits with my intuition from the mode 1 NuLA slideshow stuff I did, where exomiser did seem to slow things down but was worth it as getting more images on the disc was a big win. Since saving disc space is useful here, I can probably afford to go up to lzsa2 compression, unless the decompression code is much bulkier.
- the compressed version takes 2.80 seconds to load and display the splash screen. *LOAD of the SPLASH executable takes 1.79s. It's a shame we can't use the faster decompression code but we need backwards decompression because the screen RAM is at the top of memory and we don't have enough RAM to not overlap the buffers. Still, this isn't bad, it is saving disc space (8.5K after we allow for sector size rounding) and it is slightly faster than loading the raw uncompressed file. (I am using lzsa2 optimised for ratio; I suppose I could push things a bit to improve the performance, but saving disc space is good too and given the compressed version is still faster than uncompressed I'm not too inclined to sacrifice disc space for a tiny bit of speed.)



M128+normal 2P, --no-turbo $0029ff
M128+normal 2P, (turbo supported but not in use of course) $002a2b


B 144K SWR $0047ef
no-dynmem-adjust $004b14 - if this is to be believed a big improvement from the dynmem promotion

256K turbo, M128 host, mode 7, drive noises off, no-dynmem-adjust $00244b ($00244a)
ditto but with default dynmem-adjust $0021ec ($0021ec) - so 6.5% faster due to more memory access going via "dynmem" not "vmap"
4MHZ "turbo" copro just for fun, otherwise ditto $0023f8 ($0023f9) - so with the dynmem adjust, the 3MHz 256K turbo beats the 4MHz 64K "turbo"
3MHz 64K copro $2b5c - so 21.7% faster with a 3MHz 256K
- OK, looking at tubes[] array in b-em model.c, it looks like the turbo copro is being clocked (correctly?) at 4MHz - but even so, as per the "just for fun" result above, there is an improvement, it's just not as big


b-em, 256K turbo co-pro, no host cache, mode 7, drive noises off - $002300
Normal 6502 co-pro, host cache (144K model B), $0022bf
Ditto but host cache 64K M128 $002b2e
Ditto but host an unexpanded model B $0029fd
- so at least for the benchmark a turbo-co pro doesn't help much if you have plenty of host cache (which is maybe a testament to how fast copying data over the tube can be, combined with the fact that at least for the benchmark there's a fair amount of vram cache so we can probably maintain a pretty decent working set and don't need to hit the host cache much) - I suspect for larger games the turbo copro might show more a gain, because there's essentially no overhead to access 128K read-only memory plus maybe 40-50K (guessing) of dynamic memory, plus you can still (once I implement it) get host cache benefits on top of that

b-em, drive noises on, BBC B, 8271, DFS 0.9, 16K sideways RAM - benchmark completes at - no, it locked up in DFS code (&8CD7-DA) - forgot to note move number, *maybe* 251 but don't trust that
DFS 1.2 - no, locked up at move 253, again in DFS (ACAE/ACB1)
OK, let's turn tube on (then I can compare this with BeebEm, which can't handle BBC B no shadow), still DFS 1.2 (16K SWR has disappeared but that's a b-em glitch I think) - OK, it has locked up at move 380 - so I am thinking this is the b-em (?) problem I noticed a while back - it's stuck in DFS again at ACAE/ACB1 - yes, with BeebEm, B with 2P, DFS 1.2, no SWR completes fine, so I'm not going to worry about this too much

New build system:
- I might have to rewrite in ruby eventually but I want to start from a good Python version if I do that
- need to be careful not to make it too clever
- I need it to be semi-modular so e.g. you can do a tube-only build and the loader still works. Ideally I also want to be able to do "special config X" (where X might be "Integra-B shadow RAM for game data" or "Watford sideways RAM on a B" or "AQR") builds where the user says "I'm happy for the build to support only X not other things" and we can to some extent assume the presence of X instead of trying to auto-detect it in ways that might be unreliable. Even more ideally it would be possible (where auto-detection is feasible) to add those config X builds to a disc supporting more vanilla configs, but that isn't a high priority unless it can be supported easily. I would imagine that a build for X would support plain X and X+tube though, unless of course you specified tube-only.
- the loader BASIC code should maybe support a #ifdef style mechanism rather than the build script substituting in fragments of BASIC which rely on knowledge of the loader code. This is probably clearer, cleaner and simpler - especially if we are making the loader support some configs not being present on a particular image. Apart from conditionally including/excluding blocks of code, I could also use ${FOO} macro-ish substitution to include filenames for binaries which would allow the build script to indicate which surface of a DFS disc they were on.
- DFS/ADFS image generation should probably be handled via classes which don't have to be full-featured but which present a "nice" interface which could in theory be extended to be full-featured, without being overly ornate. I suppose really the important thing is they're clean-ish. They should probably have some kind of common interface of course.
- Handling of double-sided DFS disc images is currently rather hacky. Maybe the DFS image should wrap *both* surfaces rather than having separate images per surface, not sure though.
- It would be nice to be smarter about distributing the miscellaneous files over both surfaces of a double-sided DFS disc.
- It would be nice (but not essential - C64 build system doesn't do this) if we could automatically upgrade to a double-sided disc if the stuff won't fit on a single-sided disc. Doing things inside a function rather than all at the top level might be all it takes to support this easily.
- I should probably at least start by trying to rewrite/tidy up bits of the existing build script in order to avoid the temptation to bash out shoddy code to "get things working", which is how I got into this mess in the first place. :-)
- the really core thing that needs to be flexible and clear is building of executables
  - need to be able to handle the "try this, then try that, then try this, then compared that one and that one, then tweak it, then pick that tweaked on" stuff in as clean and general a way as possible
  - I do like the idea the current code has of having an Executable object which represents the result of trying to build something, and that it can be None if we couldn't (or decided not to, e.g. the user said not to both) build something
  - I would like CACHE2P/FINDSWR to be Executable objects themselves
  - we might potentially build up a "pseudo disc image" in memory by adding Executable objects to a list, then towards the end we would turn that into a set of add_file() calls on actual DFS/ADFS disc image objects, at which point we could potentially handle "which side shall I put this on?" for DFS double-sided more easily (e.g. always put !BOOT and LOADER on side 0, but all the other non-data files go on whichever surface has most free space, and the loader BASIC program uses ${FOO} variables for filenames so it knows where they are - we'd want to put FINDSWR on first as it's always used on startup by loader, so it might always go on side 0, and we'd probably want tp ensure CACHE2P comes "between" the loader and the Ozmoo2p binary, it might be that we treat those two as a unit and always put them (in that order) on the same surface.
  - having this "psuedo disc image" which is just a list of Executable objects would also probably be helpful in being able to sum up the sector-rounded sizes and saying "OK, this will not fit on a single-sided disc, let's automatically expand to double-sided before we start"
  - we do have a general pattern of having sets of assembly arguments which serve as a base, then we add extra options, then those form a new base with variations on them being tested
  - we have a general pattern of needing to generate label/binary/report names for variations on an executable by subtitution - if we can automate this in some way it would probably simplify and clarify quite a bit, I don't want to end up with super-verbose names but the names are not all that critical (though ideally they would be user-intelligible not e.g. just assigned some arbitrary integer values during the build) - perhaps we would just have standardised template to use e.g. "ozmoo_${VMEM:vmem}_${ACORN_SWR_BIG_DYNMEM::_sdyn}_${STARTADDR}"
  - doing the necessary extra build and chop-off-0 and append relocations work for ACORN_RELOCATABLE might also be something we could wrap up
  - when building up -D options, we should probably distinguish "Ozmoo executable args" like "-DTRACE_VM=1" from general args which might apply to FINDSWR/CACHE2P binaries - not a big deal, but keeping this cleaner is probably going to help share Executable objects for all executables not just Ozmoo variants
  - in general, it needs to be "normal" for building an Executable to fail and not require lots of exception-catching boilerplate - maybe this means there's a build() fn which may return None, or if (there probably isn't) there's any useful information from a failed build *or* (maybe the case) it's convenient to store "input" to the build in an Executable object, Executable objects may need to have a "failed" state they can be in, though I'd rather avoid this if I can
  - just maybe it's worth having the build script cache Executable objects so if we end up doing the same build multiple times we know (same command line arguments == same output, there's no randomness) we don't need to go and rebuild it
- just possibly I should work on the basis that I *won't* be able to tokenise BASIC at build time and therefore I can only do "simple" slice-and-dice type operations on the loader (e.g. delete chunks of lines based on walking the tokenised code line by line and recognising the contents of REM statements which contain #ifdef or whatever) and won't be able to generate arbitrary code in the loader - I'd rather not have this, but it *might* help avoid problems if I can't use beebasm in the final version - it may also be easy-ish to change variable names or the values assigned to variables in tokenised BASIC
- *maybe* I should generate 7-bit ASCII for the loader in order to avoid the hackery needed to treat it as binary on Python 3 - if I use the equivalent top-bit-clear codes for graphics characters this needn't bloat the BASIC code much, we'd still need to use CHR$() or VDU for "0-31" (which are treated differently by the OS and don't get put directly into screen memory; remember the SAA5050 is actually a 7-bit chip IIRC) i.e. 128-(128+31), but that's mainly the odd colour code which isn't going to account for a huge amount of characters even if someone does a custom graphics title page.
- maybe I should write a "word wrap with optional colour code prefix and blank space indent" procedure in the loader and use that for output rather than the current hacky "count spaces to make a simple string print over multiple lines correctly" approach - apart from being generally nicer, it helps make things work nicely when auto-generating code.
- maybe all of our command line arguments should be automatically converted into "#ifdef"-usable form in the loader, so that the build script doesn't need to special case things like AUTOSTART and DEFAULTMODE
- I should probably not make any attempt to parse disc images as input - the only possible reason to do this now is (I think) to get the tokenised BASIC generated by beebasm, but since that's the *only* file on the generated disc I think we can simply chop off the first two sectors and be done with it. (We could have a helper fn which wraps beebasm used as a BASIC tokeniser, then it would be easy to substitute an alternative tokeniser if desired; we aren't using beebasm as an assembler at all.)



Extending tube host cache to use SWR:
- I will move the "what SWR banks do we have" logic from the loader into findswr machine code DONE
- as part of that, I will make the count/list of banks output from the loader at $900-ish at a fixed address (I can get rid of the current fixed address output of course) DONE
- I hope all this doesn't cause loader to no longer fit in $900-aff, if it does I may need to take some steps of some kind DONE
- loader (in a cache-enabled build) will use an OS call to read those fixed addresses to show SWR details even if a second processor is detected DONE
- cache2p binary will use those fixed addresses (on the host, where it is anyway of course) to control its own SWR use


Timings with initial crude tube host cache prototype:
- with &2B blocks of host cache, b-em, mode 7, 3MHz copro, Master 128 host, drive noises on $003131
- with non-caching code (current acorn-wip branch, as released to stardot) $003f25
- OK, with &2D blocks of host cache and preloading implemented to use the host cache (the preload time *is* included in the final benchmark time, remember I am not subtracting off the initial time) $002ef8
- currently (with probably-good preloading) getting $2ef6
- with relocating cache, a B with DFS+ADFS completes in $2e8e and Master 128 in $2e28
- now with sideways RAM support, Master 128 does no disc access during game and completes in $2ae8
- with alpha 2.8 on dev MAME, an Electron with a 3MHz copro and 48K SWR completed benchmark in (approx; I started timing a few seconds in) 4:35-4:45, as measured by stopwatch (mode 6)


restart and checking for game disc:
- you're not "supposed" to not have the game disc in (at the moment)
- at least on DFS, if you take the game disc out except when you're told you can during a save, the game is likely to crash as DFS reads a block of game data from your non-game disc
- in some ways restart is no different, if you have sneakily left the wrong disc in you will get a crash then too (plus this is a pretty benign crash, as you wanted to restart *anyway*)
- I have some sympathy with the idea that a "knowldgeable" user (or just possibly we might *tell* the user if this is true) knows the game fits entirely into RAM and decides they can leave their save game disc in the drive all the time
- but as it stands the game will currently be checking for re-insertion of the game disc after a save, thwarting their attempt to be clever
- so I am thinking *either* we say "the game disc is meant to be in the drive except during saves" *or* we explicitly support a "you have enough RAM, you can remove the game disc after loading" feature and that disables the check for the game disc after a save
- if we expect the game disc to be in the drive all the time except when saving, restart doesn't really need a check for the game disc to be in
- if we sometimes allow the user to remove the game disc with our permission due to having lots of RAM, we should then do a check for the game disc on restart
- not saying I won't do any (modest) amount of work to support "game fits entirely in RAM" case, but I don't believe anyone who's given me feedback on stardot has that much RAM - short of AQR support (which I don't have yet), you'd probably need approximately 96K of sideways RAM (because there's maybe 16K of main RAM as well) to be having a chance of this occurring, although I suppose smaller games like Calypso do make this a bit more likely. "Game fits entirely in RAM" on SWR is kind of equivalent to "no vmem needed" on tube (except you can detect the latter at build time), so whatever I do I should make those behave equivalently.



Electron timing notes before and after change to allow the "wasted" main RAM to be used:
32K sideways RAM, no main RAM used - benchmark $00beca 
Experimental "use main RAM" code + 32K sideways RAM $00bb45 second try $00bb0f
- virtually no difference but I suspect interrupts get disabled a lot and the self-timing can't be trusted
Experimental - $1ae shown early on - I started a stopwatch when the screen cleared and intro started to scroll 
- move 95 at 4:05 approx
- move 228 at 9:36 approx
- move 340 at 12:42 approx
- new game prompt at 17:34 (move 449) - showed $00bb3c
Back to pure 32K sideways RAM, no main RAM used, stopwatch
- $1a3 shown early on
- at 4:05 on move 79
- move 95 at 4:43 approx
- at 9:36 on move 196
- at 14:15 on move 340
- new game prompt showed $00bec7 at 19:37
Conclusion
- so internal timings show <2% reduction in time from the extra ~5K (remember the benchmark text takes up a couple of K), while the stopwatch timings show a nearly 10.5% reduction (from a 15% increase in RAM)




load_suggested_pages performance:
- no drive noises (which would in reality dilute any overhead further, as a percentage)
- B+ 128K DFS mode 7
- current load_suggested_pages code has $2b0 at start of benchmark (confirmed)
- alpha 2.4 tag has $2af (confirmed)
- so there's no real difference, which is what I'd expected/hoped but good to confirm the bit of extra work being done between each readblocks call is not a significant overhead



Testing checklist:
- ADFS vs DFS
- single vs double-sided
- all four executables
- Python 2 vs Python 3
- small non-VMEM games on second processor as well as large VMEM games
- BBC B executable at &E00, &1900 and &1D00 (screen hole makes it very sensitive)
- really big game like Beyond Zork

Benchmarking:
- THIS IS BIGDYN BECAUSE NOT TAKEN ANY STEPS TO AVOID IT
- Master 128, mode 7, 64K sideways RAM, b-em, drive noises *off* $004988
- ditto but model B with 144K sideways RAM (so no drive access and maximum paging) $004a4e conf
- now adding jsr-to-rts in paging code
- M128 $004e69 1.0664
- model B 144K $004f12 $004f11 1.0641



Electron:
- as per git commit notes, "64k" auto-detected SWR build hangs on move 382 of benchmark
- forcing LOADER to only recognise one bank of SWR a) it is much slower (which is good, as it suggests we are "correctly" using multiple banks on the whole) b) the benchmark completes (FWIW showing time $019b41)
- forcing LOADER to take the first two banks of SWR identified (I think 3 & 5, but not sure) it hangs *much* earleir
- forcing LOADER to use just banks 3 and 7 - OK, I think that hangs in the same place as with 3 & 5, much earlier, at move 50
- OK, thanks to Pernod on stardot using Pegasus 400 DFS everything works!



Vmem notes (mostly looking at block aging stuff):
- I'll assume Z3 in these notes, where vmem_tick_increment == 2 and vmem_highbyte_mask = $01, but except for reduced resolution I don't think other versions are significantly different
- vmem_blockmask is always $fe=254 - this is used to allow for the fact a VM block contains two pages
- vmem_tick starts off at $e0
- vmem_tick doesn't move unless we need access to a block of data and it isn't in our VM cache, i.e. we need to read a block from disc and we therefore need a slot in our VM cache to store it in.
     - by this point we have already picked out the "oldest" entry so we can re-use that slot
        - in terms of picking out the oldest, we start looking at vmem_clock_index and entries have to be strictly older to replace the previous oldest candidate
        - vmem_clock_index is set to the index following the oldest candidate ready for the next search - I think doing this rather than always starting at index 0 might have some kind of effect on tie-breaking when multiple entries share exactly the same time stamp, but TODO: I really need to think this through - OK, recheck this later, but I think this is really a performance optimisation - if we started at 0 every time, we'd check lots of entries that were unlikely to be the oldest - no, but we always go round the full vmap *anyway*, so that can't be it - I suppose it does have an effect of spreading things out - if we always started at 0 blocks with joint-oldest age near 0 would always be paged out in preference to equally old blocks further down the map, whereas always starting at clock_index will tend to be "fairer" - I can see some value in this but it's not obvious to me that it is automatically a big win as I write this
    - vmem_tick is incremented by vmem_tick_increment
    - if vmem_tick wraps round, it gets set to $80 and the vmap entries are adjusted - the adjustment subtracts $80, if the result is negative the value is set to 0 (with an adjustment to preserve the non-tick-related high byte using vmem_highbyte_mask)
    - the newly read block is given the new value of vmem_tick (although debugging code will print the older value)
- if we need a block of data and it's already in the VM cache, vmem_tick does *not* change but that block's vmap entry is set to the current value of vmem_tick





Preopt and related stuff:
- I think the basics of this are not a big deal - start with an empty vmap, populate it and dump the contents when it's full
- the build system then needs to pass (presumably in order of first use) this into the binary so it can load the relevant blocks - the binary should probably sort these itself so as to seek efficiently across the disc, we can't sort them in the build system because only the binary knows how much RAM it actually has to work with, it may end up truncating the list of suggested blocks (e.g. suppose the preload blocks are 5, 6, 130, 4 in that order - if we have >=4 blocks of VM cache we want to load 4, 5, 6, 130, but if we have 3 blocks of VM cache we want to load 5, 6, 130 not 4, 5, 6 which we would do if the list were pre-sorted before truncation - note also depending on exactly how the setting of the initial age works, we may need to be careful not to lose information by doing this sort in order to do efficient loading)
- this may mean the binary needs to be capable of generating its own vmap, including doing linear interpolation or similar of the age for the entries it is generating - however, this is maybe awkward because although there is some possibility of "early" discardable init code running from story_space, the actual space in the Ozmoo stack is already pretty full - swr_shr_vmem has about 134 bytes free - other variants have more, maybe something could be moved out of the stack or optimised or whatever, but this does need consideration - adjust_dynamic_memory_inline is probably a modest chunk of the code taking the stack space on builds which have it, which isn't to say it's not worth it
- I have had a bit of a poke at the C64 code but I haven't studied it all that thoroughly and I may be missing something
- if the binary is now generating its own vmap, it may well be useful for the vmap to live in non-initialised RAM in 400-800 (something else can move out of there to compensate)
- if the vmap is going to live in non-initialised RAM, it may be a good idea to pick up the aborted "restart only reloads dynamic memory" change and preserve the vmap across restarts - although this is a bit debatable, as if you're restarting you might well want all the "start of game" blocks loaded in one go rather than what you happened to have - then again maybe you're restarting just to do a restore, but usually you would just do a restore of course
- it's possible an Electron implementation would affect generation of the vmap, as it may want to generate one with a "hole" in it for the 8K screen (to allow the ~4K below the screen to be used as VM cache), so I may want to postpone any implementation of any of this until I've decided if/how I am going to support the Electron.
- so sketching out (and ignoring Electron, which is probably not a big deal but let's keep it simple) how this would work:
	- the loader puts the suggestions into the vmap in the standard format (most "important" with newest ages, i.e. the ones loaded first during the preopt generation run which were *oldest* in that run and youngest in the vmap created by loader) - the list of suggestions is padded up to the full vmap size with missing and/or linear blocks (not too important, but they should probably be useful-ish entries as Acorn code doesn't I think like/want to grow vmap table, it starts off full)
	- the Ozmoo binary needs to count up how many blocks of vmem cache it can actually support - it probably does this already actually, as we need to set vmap_max_entries accordingly
	- the Ozmoo binary truncates the list of suggestions - this may happen implicitly as we set vmap_max_entries
	- the Ozmoo binary sorts the truncated list *using the raw game block address, not the address-with-timestamp* - this can't be done in the loader, because where the list gets truncated will affect the result - this isn't a big deal, but just note that the sort key is the value in vmap with the timestamp masked off, but of course we want to retain the full vmap entry with timestamp as we swap the blocks into order
	- the Ozmoo binary uses something like load_blocks_from_index to pull each vmap entry into memory, instead of just using readblocks to read from story_start up to the end of SWR (not needing to care about what's dynmem and what's vmem preload)




"Free memory":
- on all builds "a lot of" page 4 is really free (~113 bytes)
- non-2P builds have page 7 completely free
- we have scratch page on 2P and double scratch page on non-2P, we do need these but we could maybe use a bit more
- if necessary to free up "big" chunks of non-initialised low memory, we could of course move stuff out of there into the main Ozmoo binary
- there is also the up-to-511 bytes of padding to ensure the stack starts with the right alignment, though of course this is hard to use flexibly since it may disappear and we always need (even if it's just manual fiddling) some sort of fallback, and if we have somewhere to fall back to why would we not just use it all the time anyway?

Possible uses of free memory:
- extend game_data_filename space from 32 bytes
- maybe relocate part of vmem map into it - this needs to be initialised though
- maybe relocate some/all of 160 bytes of ACORN_HW_SCROLL buffer into it
- there's about 57 bytes of misc data scattered round the code which might be relocatable into this space, but that creates a bit of a gratuitous difference from C64 as it's mostly "upstream" data



ADFS:
I think we need to do *DISMOUNT before entering the save/restore prompt
Probably also necessary/a good idea to close the game data file before doing *DISMOUNT
When we're about to resume, we probably need to do *MOUNT, try to open the file (and if we succeed do the DFS-style CRC check), catch errors (making sure we close the file if we opened it, and do a *DISMOUNT as well) and repeat the whole process after the user presses a key.
- ****BUT**** *DISMOUNT will lose the current directory - which is not a good thing if we've been installed on a hard drive in some random directory. So maybe we need to detect that case and not fiddle with *DISMOUNT/*MOUNT if so. But what if the user goes and does *MOUNT 4 to save a game to a floppy? Admittedly this is not all that likely, but let's pretend. Will that mean we can't re-open our "DATA" (with no drive/dir prefix) file? I guess on a hard drive installation, if we're going to have to detect that case, we would simply keep the file open during a save/restore.
- can we detect hard drive? do we really want to? should we maybe instead keep the game data file open, but be ready to experience a "Channel" error when we do the CRC read and if so do *MOUNT and re-open it? But if we haven't done *DISMOUNT (and we're not trying to behave different on hard or floppy drive), the user has to go and do a *MOUNT manually if they've changed the disc, whereas if we *DISMOUNT they don't have to.
[FWIW, *is* (genuine Q) OSARGS A=&FF equivalent to *DISMOUNT? Might be useful, might not.]
[I don't know if there's an official way to detect a hard drive, *if* I want to go down that route, but reading the capacity - not free space - of the current drive would in practice be a good way to detect this. No hard drive is going to be <=640K.]

How about this?
- we try very hard not to implement an "ADFS" solution - we might in principle be running on some other filing system like Econet or something more exotic.
- so we don't go issuing *DISMOUNT or *MOUNT commands if we can possibly help it
- the loader determines the current path and stashes the full ":0.$.Foo.Bar.Ozmoo.Data" string somewhere in memory - note the drive number (awkward? what if we're on a file system without drive numbers?)
- the game closes the data file before save/restore so that *if* the user does something like *MOUNT 0 to change the disc, we don't get upset at having the file closed underneath us
- after the save/restore is complete the game re-opens the data file using that full path - this way if the user has deliberately or accidentally changed the current directory to wherever they want to save (or maybe they hacked the loader to do this, if they have a hard drive installation) we won't get upset or lose their setting for the next time they save/restore
- what if the user has taken the game disc away? Our open might fail with a Disc changed error or a Not found error, or the CRC check once we open it might reveal the disc isn't the one we want. Ideally we would offer a * prompt to let the user "do stuff" to fix the situation before retrying, we could maybe get away with issuing a *MOUNT 0 command.
- RESTART is maybe going to be problematic as we need to launch the binary from the right path and if the user's changed it we will fail

Alternative:
- maybe the loader stashes a command or commands somewhere in memory to do "post save/restore", and the loader tries to set something sensible up and an advanced user (i.e. one installing to hard drive or Econet or whatever) can tweak this if the loader doesn't do the right thing automatically, as the loader is BASIC.
- the loader might similarly stash a full restart path
- apart from executing the post save/restore commands, the game would "just" a) close the file before the save/restore b) open the data file using a stashed-by-the-loader path, check its contents via CRC. If b) fails, retry - although ideally we still need a command prompt loop.

So on save restore:
- close the file
- tell the user they can remove the game disc (but ideally don't do that if e.g. we're running from a hard drive)

After save restore:
- I don't think we can "just try" to see if the game disc is already there, as we can on DFS, because it's got a fair chance of causing some sort of "Disc changed" or "Not mounted" error and the user had no chance to do it - hmm, maybe we can/should *recognise* Disc changed and swallow it but "do something"???
- perhaps we can "just try" by doing an OPENIN on the game data file - it's always possible no disc is mounted, but probably only if the user did something and semi-invited this

If we *are* on ADFS using floppy:
- we need to close the game data before saving
- ideally we would *DISMOUNT too (this may avoid need for user to *MOUNT explicitly)
- we would tell the user they can remove the game disc
- they do * commands and save as normal
- once they've finished, we *ask* them to put the game disc back in and press SPACE - unless we can try to OPENIN the game data file and swallow any error - OK, so we try the OPENIN, if it succeeds we should check the CRC before considering it to be OK, if it fails we swallow the error and the user to put the game disc back in, at which point we do *MOUNT 0 ourselves and retry - I think we need to be opening the data file using a full path, so if the user did *MOUNT 1 we won't get upset

OK, let's look at it this way:
- if we're on a hard drive or Econet, it's "easy" - we just keep the game data file open (we need to be careful re path on restart, but that's all) and unless the user goes an *DISMOUNTs the hard drive or something we're fine. We may need to *not* do some stuff we would otherwise do for floppy (actual commands, or maybe printing messages about "removing/re-inserting the game disc"), but it's not really a big deal. It might be nice to allow * commands if things go wrong, but this will probably just fall out of stuff we need to do for floppy anyway.
- conceptually being on a hard drive but saving to a floppy is no different than being on a disc in drive 0 and saving to drive 1 - the user will be using *MOUNT some-other-drive, they may accidentally *DISMOUNT the drive we are on, etc.
- playing around on emulated master, using OPENIN (at least with a full path) appears to implicitly cause a mount, whereas LOAD "A" after a dismount gives a "No directory" error - the full path does appear to be the magic ingredient, it's not e.g. OPENIN vs LOAD which is important. Hmm, OPENIN does sometimes seem to work when LOAD doesn't, it's really inconsistent. But a full path with a drive number does appear to generate provoke ADFS to "have a go" rather than say "No directory". Also, of course, OPENIN can "fail" quietly by returning a zero channel, which I suspect is a key observation here. Giving the full path does appear to sidestep the whole "disc changed" error in general though, full path with drive number that is. This may well simplify things, if the loader obtains a full path with drive number and stashes it somewhere, we may be able to do open-read-block-0-and-crc without *expecting* to get any OS errors occurring, though we should ideally cope if they do.

For the nth time then:
- prior to save/restore we close the game data file
- if we're "on floppy", we *DISMOUNT and print "you can now remove the game disc" - hmm, this *DISMOUNT is nice in that I think it allows the user to swap the disc without needing to *MOUNT, *but* if the user is actually saving onto the game disc but would like to use a non-root directory, the *DISMOUNT will put them back in the root directory every time - so maybe we don't do the *DISMOUNT, at which point printing the "you can now remove the game disc" is a harmless and understandable quirk if the game is on hard drive, and we maybe don't even need to try to distinguish the two cases
- we let the user enter * commands etc as usual and do the save/restore
- we then re-open the game data file using a full path including a drive number - this may return a zero file handle, or we may read the data from the file and find the crc doesn't match - in either case we close the file, ask the user to put the game disc in and press space and then we try again - we may need to be careful here because the readblocks code will perhaps want to get in on the act with the retry, but we may want/need to be doing it ourselves - if we made readblock automatically close and re-open the file on error, it may well be we would not have any worries there
- if this model works, the loader needs to pass a) the full path of the game data file b) the full path of the Ozmoo binary (for restarts) to the Ozmoo binary somewhere in memory - we need this on all builds, not just SWR of course - if these paths are not allowed to be insanely long (the loader could object) we could probably squash them into page 4 and shrink the jmp_buf a bit more - the Ozmoo binary could have code to copy a single path (of the game data file) into the scratch page and hack the last bit after the "." to be its own executable name and do the *RUN from there, rather than the us needing to allocate two bits of memory for duplicate paths - this is a bit fiddly (=> more resident code), but it would also offer flexibility for DFS versions to have the executables on "arbitrary" sides of the disc, as long as the loader knows where they are




Benchmarks after reworking sideways RAM paging:

Baseline is 87f85b82608f6072ebeaebd0dfde488931e00c42 acorn-swr branch latest - this always uses "old-style no tweaks bigdyn":
Using b-em Master 128 mode with 64K sideways RAM in mode 7 for all tests
Drive noises off: $004d84 (repeat $004d83)
Drive noises on: $0051c2 (repeat $0051c2)

Comparison is latest acorn-wip 30ebfe64c647828c2716e0e3b49027d80479af11
bigdyn: 
Drive noises off: $0049a4 95.0% (repeat $0049a5)
Drive noises on: $004e0b 95.5% (repeat $004e0b)

smalldyn:
Drive noises off: $0045c0 90.0% (repeat $0045bf)
Drive noises on: $004a00 90.5% (repeat $004a00)




Register use analysis:

read_byte_at_z_address:
- returns value read in A
- upstream code looks like X has a pretty arbitrary value on exit
- upstream code will always (?) return with Y=0 but I don't believe any caller relies on that
- called by get_page_at_z_pc{,_did_pha}
- called by .initial_copy_loop in C64 disk code, but that immediately trashes A, X and Y after call
- called by read_next_byte, which explicitly preserves its caller's Y and doesn't use the Y=0 return
- called by .copy_table_common, which immediately does ldy #0 after calling it and trashes X a dozen or so instructions later
- no other callers
- so I think this is allowed to corrupt X and Y

get_page_at_z_pc{,_did_pha}:
- contains upstream code to explicitly preserve A and X
- upstream code explicitly returns with Y=0 and has a comment saying this is important
- calls read_byte_at_z_address
- called by C64 restore_game, which immediately does lda and ldx afterwards - there is no clear use of Y in the following code, and I am 99% confident it doesn't matter - it does an rts after which will go via make_branch_{true,false) and as discussed under read_next_byte_at_z_pc those will return control to the main loop
- called by inc_z_pc_page - this is used only by read_next_byte_at_z_pc and so I am pretty confident (see notes about that routine) that it is not necessary to have Y=0 for that
- called by z_init, which will trash A, X and Y not longer afterwards
- so I think this can maybe corrupt Y, provided read_next_byte_at_z_pc takes its own steps to ensure Y=0 (and I'm not sure that's actually necessary)

read_next_byte:
- returns value in A
- upstream code explicitly preserves caller's Y
- calls read_byte_at_z_address
- upstream code will return with the X value from read_byte_at_z_address, which is pretty arbitrary
- has many callers, not analysed
- so I think this is allowed to corrupt X

inc_z_pc_page:
- upstream code explicitly preserves A
- X and Y are altered iff get_page_at_z_pc_did_pha is called and alters them; it explicitly preserves X, so inc_z_pc_page will also preserve X, and it may (depending on whether get_page... is called) set Y to 0
- called only by read_next_byte_at_z_pc

read_next_byte_at_z_pc:
- returns result in A
- upstream code will always return with Y=0, because it sets Y=0 and *maybe* calls inc_z_pc_page, which will also set Y=0
- will preserve X
- has many callers, some of which rely on it preserving X
- I can't see any callers obviously depending on Y=0 but I can't be 100% sure there isn't something I'm missing - I am 95% confident having looked at the callers this isn't a problem, and in practice it really doesn't seem to break anything having Y!=0 on exit - OK, I've been over the callers in more detail, I am now 99% confident - the only possibly iffiness is make_branch_{true,false}, but I am pretty sure those return control to the main loop at rts and except vaguely-possibly for the not_normal_exe_mode case the main loop trashes Y fairly early on












On C64 the game normally keeps I/O+ROM paged in from $D000 upwards; copy_page
is used to copy RAM around with RAM paged in from $D000 upwards. The vmem_cache
stuff keeps a copy of high RAM pages currently in use in low RAM. This is
accessible RAM caching inaccessible RAM; it has nothing to do with optimising
disc access. On Acorn second processor builds this is irrelevant as there's no
paging. SF: I suspect this is also irrelevant on Acorn SWR builds, but I
need to investigate exactly how Z-machine non-dynamic memory is accessed to see
if we can ensure the correct SWR bank is paged in at all times without using
this caching mechanisms.

mempointer is read in get_page_at_z_pc immediately after calling read_byte_at_z_address - it is copied into z_pc_mempointer+1
ditto: in initial_copy_loop (REU code)
read_byte_at_z_address updates mempointer; a fast path relies on mempointer+1 being consistent with zp_pc_[lh]
read_byte_at_z_address updates zp_pc_[lh]
read_next_byte uses read_byte_at_z_address to read the address at z_address+[012], it then advances z_address+[012]
copy_table_common uses read_byte_at_z_address
zp_pc_[lh] are really only used in vmem.asm instead read_byte_at_z_address; I think they basically boil down to "what page of Z-machine memory does mempointer currently point to?" as an optimisation.

I'm struggling to see why we have both mempointer and zp_pc_mempointer.
I *think* zp_pc_mempointer is used for the Z-machine program counter, and this may be allowed to be "separate" to mempointer. I see that the *cache* support refuses to evict a cache page if zp_pc_mempointer is referring to it, but I don't see anything in the disc<->RAM stuff which would stop a vmem page being discarded if zp_pc_mempointer refers to it.
Yes, I think that's basically it
- mempointer is used for data access, and is also moved in the process of updating zp_pc_mempointer but that's OK because we don't rely on mempointer pointing to any one place except in the short term
- I still can't see anything protecting vmem pages being used by zp_pc_mempointer being evicted, although I suppose short of extreme memory pressure (which we might see on a hypothetical 32K RAM port) this won't happen - I could potentially tweak the vmem eviction code to check against zp_pc_mempointer - one possible way to do this easily would be just under .no_such_block to set the tick on the vmem block containing the PC (we could keep a copy of the index when we do the new-page stuff for z-pc) to a very recent value - then again it might well be as easy just to tweak the chosen code - ah no, it looks OK (of course), just under .no_such_block we have "; Skip if z_pc points here" so this is fine
- since only (?) read_byte_at_z_address can discard pages from vmem, mempointer is always going to be safe from eviction - read_byte_at_z_address is what sets mempointer

I suspect on a SWR build, we would keep the bank containing z_pc_mempointer paged in at all times. read_byte_at_z_address would be used for reads of data, and it would briefly page in the relevant bank. We would probably need to tweak the logic when the Z-machine PC is being moved and might call into read_byte_at_z_address so we don't get into a loop - actually it would probably be fine, but it would be silly to page back in the old PC bank just to revert to paging in the new PC bank in the caller of read_byte_at_z_address in the PC update code.
- we *could* use the C64-ish cache mechanism to avoid this, but I don't think we need to - the C64 presumably needs its high ROM banked in for kernal interrupts and keyboard reading and so forth, whereas the SWR on the Acorn isn't conflicting with the OS and we can safely keep it paged in all the time, it's just a question of ensuring we cope when the PC and "data" reads are from different SWR banks.

I think zp_pc_[lh] are *not* related to the Z-machine PC; they may be misnamed, or "pc" may stand for something else, or of course I might have the wrong end of the stick.



Loading a $C800 byte preload file using readblocks (2 pages at a time) took (b-em, drive noises on, timings via kernal_readtime):
Master Turbo OS 3.2 - 8.22s
DFS 0.9 with Turbo copro - 8.0s
DFS 2.26 with Turbo copro - 8.2s
compared with OSFILE:
Master Turbo OS 3.2 - 7.62s
DFS 0.9 with Turbo copro - 7.62s
DFS 2.26 with Turbo copro - 7.62s
- so it looks as though giving up PRELOAD and using readblocks to do the initial load (which would simplify things if I have two different-sized preloads and/or if I start interleaving across both sides of the disc) is not a significant performance hit



Biggish chunks of data for initial move to $400:
.jmp_buf max 257 bytes but in reality much smaller, not measured/analysed yet


Empirically working out .jmp_buf storage requirement: poking stack page full of $EA and running benchmark then examining stack afterwards, it looks like we never got down below $1E0 (and some of that may be interrupts below our stack, which don't count for .jmp_buf purposes).
Skimming the call code, it doesn't look like Z machine function calls build up state on the 6502 stack - this makes sense, as otherwise a save/restore wouldn't restore that state correctly. So I don't think a sufficiently convoluted program running on Ozmoo can't provoke higher-than-normal 6502 stack use.
