Testing checklist:
- ADFS vs DFS
- single vs double-sided
- all four executables
- Python 2 vs Python 3

Benchmarking:
- THIS IS BIGDYN BECAUSE NOT TAKEN ANY STEPS TO AVOID IT
- Master 128, mode 7, 64K sideways RAM, b-em, drive noises *off* $004988
- ditto but model B with 144K sideways RAM (so no drive access and maximum paging) $004a4e conf
- now adding jsr-to-rts in paging code
- M128 $004e69 1.0664
- model B 144K $004f12 $004f11 1.0641



Electron:
- as per git commit notes, "64k" auto-detected SWR build hangs on move 382 of benchmark
- forcing LOADER to only recognise one bank of SWR a) it is much slower (which is good, as it suggests we are "correctly" using multiple banks on the whole) b) the benchmark completes (FWIW showing time $019b41)
- forcing LOADER to take the first two banks of SWR identified (I think 3 & 5, but not sure) it hangs *much* earleir
- forcing LOADER to use just banks 3 and 7 - OK, I think that hangs in the same place as with 3 & 5, much earlier, at move 50
- OK, thanks to Pernod on stardot using Pegasus 400 DFS everything works!



Vmem notes (mostly looking at block aging stuff):
- I'll assume Z3 in these notes, where vmem_tick_increment == 2 and vmem_highbyte_mask = $01, but except for reduced resolution I don't think other versions are significantly different
- vmem_blockmask is always $fe=254 - this is used to allow for the fact a VM block contains two pages
- vmem_tick starts off at $e0
- vmem_tick doesn't move unless we need access to a block of data and it isn't in our VM cache, i.e. we need to read a block from disc and we therefore need a slot in our VM cache to store it in.
     - by this point we have already picked out the "oldest" entry so we can re-use that slot
        - in terms of picking out the oldest, we start looking at vmem_clock_index and entries have to be strictly older to replace the previous oldest candidate
        - vmem_clock_index is set to the index following the oldest candidate ready for the next search - I think doing this rather than always starting at index 0 might have some kind of effect on tie-breaking when multiple entries share exactly the same time stamp, but TODO: I really need to think this through - OK, recheck this later, but I think this is really a performance optimisation - if we started at 0 every time, we'd check lots of entries that were unlikely to be the oldest - no, but we always go round the full vmap *anyway*, so that can't be it - I suppose it does have an effect of spreading things out - if we always started at 0 blocks with joint-oldest age near 0 would always be paged out in preference to equally old blocks further down the map, whereas always starting at clock_index will tend to be "fairer" - I can see some value in this but it's not obvious to me that it is automatically a big win as I write this
    - vmem_tick is incremented by vmem_tick_increment
    - if vmem_tick wraps round, it gets set to $80 and the vmap entries are adjusted - the adjustment subtracts $80, if the result is negative the value is set to 0 (with an adjustment to preserve the non-tick-related high byte using vmem_highbyte_mask)
    - the newly read block is given the new value of vmem_tick (although debugging code will print the older value)
- if we need a block of data and it's already in the VM cache, vmem_tick does *not* change but that block's vmap entry is set to the current value of vmem_tick





Preopt and related stuff:
- I think the basics of this are not a big deal - start with an empty vmap, populate it and dump the contents when it's full
- the build system then needs to pass (presumably in order of first use) this into the binary so it can load the relevant blocks - the binary should probably sort these itself so as to seek efficiently across the disc, we can't sort them in the build system because only the binary knows how much RAM it actually has to work with, it may end up truncating the list of suggested blocks (e.g. suppose the preload blocks are 5, 6, 130, 4 in that order - if we have >=4 blocks of VM cache we want to load 4, 5, 6, 130, but if we have 3 blocks of VM cache we want to load 5, 6, 130 not 4, 5, 6 which we would do if the list were pre-sorted before truncation - note also depending on exactly how the setting of the initial age works, we may need to be careful not to lose information by doing this sort in order to do efficient loading)
- this may mean the binary needs to be capable of generating its own vmap, including doing linear interpolation or similar of the age for the entries it is generating - however, this is maybe awkward because although there is some possibility of "early" discardable init code running from story_space, the actual space in the Ozmoo stack is already pretty full - swr_shr_vmem has about 134 bytes free - other variants have more, maybe something could be moved out of the stack or optimised or whatever, but this does need consideration - adjust_dynamic_memory_inline is probably a modest chunk of the code taking the stack space on builds which have it, which isn't to say it's not worth it
- I have had a bit of a poke at the C64 code but I haven't studied it all that thoroughly and I may be missing something
- if the binary is now generating its own vmap, it may well be useful for the vmap to live in non-initialised RAM in 400-800 (something else can move out of there to compensate)
- if the vmap is going to live in non-initialised RAM, it may be a good idea to pick up the aborted "restart only reloads dynamic memory" change and preserve the vmap across restarts - although this is a bit debatable, as if you're restarting you might well want all the "start of game" blocks loaded in one go rather than what you happened to have - then again maybe you're restarting just to do a restore, but usually you would just do a restore of course
- it's possible an Electron implementation would affect generation of the vmap, as it may want to generate one with a "hole" in it for the 8K screen (to allow the ~4K below the screen to be used as VM cache), so I may want to postpone any implementation of any of this until I've decided if/how I am going to support the Electron.
- so sketching out (and ignoring Electron, which is probably not a big deal but let's keep it simple) how this would work:
	- the loader puts the suggestions into the vmap in the standard format (most "important" with newest ages, i.e. the ones loaded first during the preopt generation run which were *oldest* in that run and youngest in the vmap created by loader) - the list of suggestions is padded up to the full vmap size with missing and/or linear blocks (not too important, but they should probably be useful-ish entries as Acorn code doesn't I think like/want to grow vmap table, it starts off full)
	- the Ozmoo binary needs to count up how many blocks of vmem cache it can actually support - it probably does this already actually, as we need to set vmap_max_entries accordingly
	- the Ozmoo binary truncates the list of suggestions - this may happen implicitly as we set vmap_max_entries
	- the Ozmoo binary sorts the truncated list *using the raw game block address, not the address-with-timestamp* - this can't be done in the loader, because where the list gets truncated will affect the result - this isn't a big deal, but just note that the sort key is the value in vmap with the timestamp masked off, but of course we want to retain the full vmap entry with timestamp as we swap the blocks into order
	- the Ozmoo binary uses something like load_blocks_from_index to pull each vmap entry into memory, instead of just using readblocks to read from story_start up to the end of SWR (not needing to care about what's dynmem and what's vmem preload)




"Free memory":
- on all builds "a lot of" page 4 is really free (~113 bytes)
- non-2P builds have page 7 completely free
- we have scratch page on 2P and double scratch page on non-2P, we do need these but we could maybe use a bit more
- if necessary to free up "big" chunks of non-initialised low memory, we could of course move stuff out of there into the main Ozmoo binary
- there is also the up-to-511 bytes of padding to ensure the stack starts with the right alignment, though of course this is hard to use flexibly since it may disappear and we always need (even if it's just manual fiddling) some sort of fallback, and if we have somewhere to fall back to why would we not just use it all the time anyway?

Possible uses of free memory:
- extend game_data_filename space from 32 bytes
- maybe relocate part of vmem map into it - this needs to be initialised though
- maybe relocate some/all of 160 bytes of ACORN_HW_SCROLL buffer into it
- there's about 57 bytes of misc data scattered round the code which might be relocatable into this space, but that creates a bit of a gratuitous difference from C64 as it's mostly "upstream" data



ADFS:
I think we need to do *DISMOUNT before entering the save/restore prompt
Probably also necessary/a good idea to close the game data file before doing *DISMOUNT
When we're about to resume, we probably need to do *MOUNT, try to open the file (and if we succeed do the DFS-style CRC check), catch errors (making sure we close the file if we opened it, and do a *DISMOUNT as well) and repeat the whole process after the user presses a key.
- ****BUT**** *DISMOUNT will lose the current directory - which is not a good thing if we've been installed on a hard drive in some random directory. So maybe we need to detect that case and not fiddle with *DISMOUNT/*MOUNT if so. But what if the user goes and does *MOUNT 4 to save a game to a floppy? Admittedly this is not all that likely, but let's pretend. Will that mean we can't re-open our "DATA" (with no drive/dir prefix) file? I guess on a hard drive installation, if we're going to have to detect that case, we would simply keep the file open during a save/restore.
- can we detect hard drive? do we really want to? should we maybe instead keep the game data file open, but be ready to experience a "Channel" error when we do the CRC read and if so do *MOUNT and re-open it? But if we haven't done *DISMOUNT (and we're not trying to behave different on hard or floppy drive), the user has to go and do a *MOUNT manually if they've changed the disc, whereas if we *DISMOUNT they don't have to.
[FWIW, *is* (genuine Q) OSARGS A=&FF equivalent to *DISMOUNT? Might be useful, might not.]
[I don't know if there's an official way to detect a hard drive, *if* I want to go down that route, but reading the capacity - not free space - of the current drive would in practice be a good way to detect this. No hard drive is going to be <=640K.]

How about this?
- we try very hard not to implement an "ADFS" solution - we might in principle be running on some other filing system like Econet or something more exotic.
- so we don't go issuing *DISMOUNT or *MOUNT commands if we can possibly help it
- the loader determines the current path and stashes the full ":0.$.Foo.Bar.Ozmoo.Data" string somewhere in memory - note the drive number (awkward? what if we're on a file system without drive numbers?)
- the game closes the data file before save/restore so that *if* the user does something like *MOUNT 0 to change the disc, we don't get upset at having the file closed underneath us
- after the save/restore is complete the game re-opens the data file using that full path - this way if the user has deliberately or accidentally changed the current directory to wherever they want to save (or maybe they hacked the loader to do this, if they have a hard drive installation) we won't get upset or lose their setting for the next time they save/restore
- what if the user has taken the game disc away? Our open might fail with a Disc changed error or a Not found error, or the CRC check once we open it might reveal the disc isn't the one we want. Ideally we would offer a * prompt to let the user "do stuff" to fix the situation before retrying, we could maybe get away with issuing a *MOUNT 0 command.
- RESTART is maybe going to be problematic as we need to launch the binary from the right path and if the user's changed it we will fail

Alternative:
- maybe the loader stashes a command or commands somewhere in memory to do "post save/restore", and the loader tries to set something sensible up and an advanced user (i.e. one installing to hard drive or Econet or whatever) can tweak this if the loader doesn't do the right thing automatically, as the loader is BASIC.
- the loader might similarly stash a full restart path
- apart from executing the post save/restore commands, the game would "just" a) close the file before the save/restore b) open the data file using a stashed-by-the-loader path, check its contents via CRC. If b) fails, retry - although ideally we still need a command prompt loop.

So on save restore:
- close the file
- tell the user they can remove the game disc (but ideally don't do that if e.g. we're running from a hard drive)

After save restore:
- I don't think we can "just try" to see if the game disc is already there, as we can on DFS, because it's got a fair chance of causing some sort of "Disc changed" or "Not mounted" error and the user had no chance to do it - hmm, maybe we can/should *recognise* Disc changed and swallow it but "do something"???
- perhaps we can "just try" by doing an OPENIN on the game data file - it's always possible no disc is mounted, but probably only if the user did something and semi-invited this

If we *are* on ADFS using floppy:
- we need to close the game data before saving
- ideally we would *DISMOUNT too (this may avoid need for user to *MOUNT explicitly)
- we would tell the user they can remove the game disc
- they do * commands and save as normal
- once they've finished, we *ask* them to put the game disc back in and press SPACE - unless we can try to OPENIN the game data file and swallow any error - OK, so we try the OPENIN, if it succeeds we should check the CRC before considering it to be OK, if it fails we swallow the error and the user to put the game disc back in, at which point we do *MOUNT 0 ourselves and retry - I think we need to be opening the data file using a full path, so if the user did *MOUNT 1 we won't get upset

OK, let's look at it this way:
- if we're on a hard drive or Econet, it's "easy" - we just keep the game data file open (we need to be careful re path on restart, but that's all) and unless the user goes an *DISMOUNTs the hard drive or something we're fine. We may need to *not* do some stuff we would otherwise do for floppy (actual commands, or maybe printing messages about "removing/re-inserting the game disc"), but it's not really a big deal. It might be nice to allow * commands if things go wrong, but this will probably just fall out of stuff we need to do for floppy anyway.
- conceptually being on a hard drive but saving to a floppy is no different than being on a disc in drive 0 and saving to drive 1 - the user will be using *MOUNT some-other-drive, they may accidentally *DISMOUNT the drive we are on, etc.
- playing around on emulated master, using OPENIN (at least with a full path) appears to implicitly cause a mount, whereas LOAD "A" after a dismount gives a "No directory" error - the full path does appear to be the magic ingredient, it's not e.g. OPENIN vs LOAD which is important. Hmm, OPENIN does sometimes seem to work when LOAD doesn't, it's really inconsistent. But a full path with a drive number does appear to generate provoke ADFS to "have a go" rather than say "No directory". Also, of course, OPENIN can "fail" quietly by returning a zero channel, which I suspect is a key observation here. Giving the full path does appear to sidestep the whole "disc changed" error in general though, full path with drive number that is. This may well simplify things, if the loader obtains a full path with drive number and stashes it somewhere, we may be able to do open-read-block-0-and-crc without *expecting* to get any OS errors occurring, though we should ideally cope if they do.

For the nth time then:
- prior to save/restore we close the game data file
- if we're "on floppy", we *DISMOUNT and print "you can now remove the game disc" - hmm, this *DISMOUNT is nice in that I think it allows the user to swap the disc without needing to *MOUNT, *but* if the user is actually saving onto the game disc but would like to use a non-root directory, the *DISMOUNT will put them back in the root directory every time - so maybe we don't do the *DISMOUNT, at which point printing the "you can now remove the game disc" is a harmless and understandable quirk if the game is on hard drive, and we maybe don't even need to try to distinguish the two cases
- we let the user enter * commands etc as usual and do the save/restore
- we then re-open the game data file using a full path including a drive number - this may return a zero file handle, or we may read the data from the file and find the crc doesn't match - in either case we close the file, ask the user to put the game disc in and press space and then we try again - we may need to be careful here because the readblocks code will perhaps want to get in on the act with the retry, but we may want/need to be doing it ourselves - if we made readblock automatically close and re-open the file on error, it may well be we would not have any worries there
- if this model works, the loader needs to pass a) the full path of the game data file b) the full path of the Ozmoo binary (for restarts) to the Ozmoo binary somewhere in memory - we need this on all builds, not just SWR of course - if these paths are not allowed to be insanely long (the loader could object) we could probably squash them into page 4 and shrink the jmp_buf a bit more - the Ozmoo binary could have code to copy a single path (of the game data file) into the scratch page and hack the last bit after the "." to be its own executable name and do the *RUN from there, rather than the us needing to allocate two bits of memory for duplicate paths - this is a bit fiddly (=> more resident code), but it would also offer flexibility for DFS versions to have the executables on "arbitrary" sides of the disc, as long as the loader knows where they are




Benchmarks after reworking sideways RAM paging:

Baseline is 87f85b82608f6072ebeaebd0dfde488931e00c42 acorn-swr branch latest - this always uses "old-style no tweaks bigdyn":
Using b-em Master 128 mode with 64K sideways RAM in mode 7 for all tests
Drive noises off: $004d84 (repeat $004d83)
Drive noises on: $0051c2 (repeat $0051c2)

Comparison is latest acorn-wip 30ebfe64c647828c2716e0e3b49027d80479af11
bigdyn: 
Drive noises off: $0049a4 95.0% (repeat $0049a5)
Drive noises on: $004e0b 95.5% (repeat $004e0b)

smalldyn:
Drive noises off: $0045c0 90.0% (repeat $0045bf)
Drive noises on: $004a00 90.5% (repeat $004a00)




Register use analysis:

read_byte_at_z_address:
- returns value read in A
- upstream code looks like X has a pretty arbitrary value on exit
- upstream code will always (?) return with Y=0 but I don't believe any caller relies on that
- called by get_page_at_z_pc{,_did_pha}
- called by .initial_copy_loop in C64 disk code, but that immediately trashes A, X and Y after call
- called by read_next_byte, which explicitly preserves its caller's Y and doesn't use the Y=0 return
- called by .copy_table_common, which immediately does ldy #0 after calling it and trashes X a dozen or so instructions later
- no other callers
- so I think this is allowed to corrupt X and Y

get_page_at_z_pc{,_did_pha}:
- contains upstream code to explicitly preserve A and X
- upstream code explicitly returns with Y=0 and has a comment saying this is important
- calls read_byte_at_z_address
- called by C64 restore_game, which immediately does lda and ldx afterwards - there is no clear use of Y in the following code, and I am 99% confident it doesn't matter - it does an rts after which will go via make_branch_{true,false) and as discussed under read_next_byte_at_z_pc those will return control to the main loop
- called by inc_z_pc_page - this is used only by read_next_byte_at_z_pc and so I am pretty confident (see notes about that routine) that it is not necessary to have Y=0 for that
- called by z_init, which will trash A, X and Y not longer afterwards
- so I think this can maybe corrupt Y, provided read_next_byte_at_z_pc takes its own steps to ensure Y=0 (and I'm not sure that's actually necessary)

read_next_byte:
- returns value in A
- upstream code explicitly preserves caller's Y
- calls read_byte_at_z_address
- upstream code will return with the X value from read_byte_at_z_address, which is pretty arbitrary
- has many callers, not analysed
- so I think this is allowed to corrupt X

inc_z_pc_page:
- upstream code explicitly preserves A
- X and Y are altered iff get_page_at_z_pc_did_pha is called and alters them; it explicitly preserves X, so inc_z_pc_page will also preserve X, and it may (depending on whether get_page... is called) set Y to 0
- called only by read_next_byte_at_z_pc

read_next_byte_at_z_pc:
- returns result in A
- upstream code will always return with Y=0, because it sets Y=0 and *maybe* calls inc_z_pc_page, which will also set Y=0
- will preserve X
- has many callers, some of which rely on it preserving X
- I can't see any callers obviously depending on Y=0 but I can't be 100% sure there isn't something I'm missing - I am 95% confident having looked at the callers this isn't a problem, and in practice it really doesn't seem to break anything having Y!=0 on exit - OK, I've been over the callers in more detail, I am now 99% confident - the only possibly iffiness is make_branch_{true,false}, but I am pretty sure those return control to the main loop at rts and except vaguely-possibly for the not_normal_exe_mode case the main loop trashes Y fairly early on












On C64 the game normally keeps I/O+ROM paged in from $D000 upwards; copy_page
is used to copy RAM around with RAM paged in from $D000 upwards. The vmem_cache
stuff keeps a copy of high RAM pages currently in use in low RAM. This is
accessible RAM caching inaccessible RAM; it has nothing to do with optimising
disc access. On Acorn second processor builds this is irrelevant as there's no
paging. SF: I suspect this is also irrelevant on Acorn SWR builds, but I
need to investigate exactly how Z-machine non-dynamic memory is accessed to see
if we can ensure the correct SWR bank is paged in at all times without using
this caching mechanisms.

mempointer is read in get_page_at_z_pc immediately after calling read_byte_at_z_address - it is copied into z_pc_mempointer+1
ditto: in initial_copy_loop (REU code)
read_byte_at_z_address updates mempointer; a fast path relies on mempointer+1 being consistent with zp_pc_[lh]
read_byte_at_z_address updates zp_pc_[lh]
read_next_byte uses read_byte_at_z_address to read the address at z_address+[012], it then advances z_address+[012]
copy_table_common uses read_byte_at_z_address
zp_pc_[lh] are really only used in vmem.asm instead read_byte_at_z_address; I think they basically boil down to "what page of Z-machine memory does mempointer currently point to?" as an optimisation.

I'm struggling to see why we have both mempointer and zp_pc_mempointer.
I *think* zp_pc_mempointer is used for the Z-machine program counter, and this may be allowed to be "separate" to mempointer. I see that the *cache* support refuses to evict a cache page if zp_pc_mempointer is referring to it, but I don't see anything in the disc<->RAM stuff which would stop a vmem page being discarded if zp_pc_mempointer refers to it.
Yes, I think that's basically it
- mempointer is used for data access, and is also moved in the process of updating zp_pc_mempointer but that's OK because we don't rely on mempointer pointing to any one place except in the short term
- I still can't see anything protecting vmem pages being used by zp_pc_mempointer being evicted, although I suppose short of extreme memory pressure (which we might see on a hypothetical 32K RAM port) this won't happen - I could potentially tweak the vmem eviction code to check against zp_pc_mempointer - one possible way to do this easily would be just under .no_such_block to set the tick on the vmem block containing the PC (we could keep a copy of the index when we do the new-page stuff for z-pc) to a very recent value - then again it might well be as easy just to tweak the chosen code - ah no, it looks OK (of course), just under .no_such_block we have "; Skip if z_pc points here" so this is fine
- since only (?) read_byte_at_z_address can discard pages from vmem, mempointer is always going to be safe from eviction - read_byte_at_z_address is what sets mempointer

I suspect on a SWR build, we would keep the bank containing z_pc_mempointer paged in at all times. read_byte_at_z_address would be used for reads of data, and it would briefly page in the relevant bank. We would probably need to tweak the logic when the Z-machine PC is being moved and might call into read_byte_at_z_address so we don't get into a loop - actually it would probably be fine, but it would be silly to page back in the old PC bank just to revert to paging in the new PC bank in the caller of read_byte_at_z_address in the PC update code.
- we *could* use the C64-ish cache mechanism to avoid this, but I don't think we need to - the C64 presumably needs its high ROM banked in for kernal interrupts and keyboard reading and so forth, whereas the SWR on the Acorn isn't conflicting with the OS and we can safely keep it paged in all the time, it's just a question of ensuring we cope when the PC and "data" reads are from different SWR banks.

I think zp_pc_[lh] are *not* related to the Z-machine PC; they may be misnamed, or "pc" may stand for something else, or of course I might have the wrong end of the stick.



Loading a $C800 byte preload file using readblocks (2 pages at a time) took (b-em, drive noises on, timings via kernal_readtime):
Master Turbo OS 3.2 - 8.22s
DFS 0.9 with Turbo copro - 8.0s
DFS 2.26 with Turbo copro - 8.2s
compared with OSFILE:
Master Turbo OS 3.2 - 7.62s
DFS 0.9 with Turbo copro - 7.62s
DFS 2.26 with Turbo copro - 7.62s
- so it looks as though giving up PRELOAD and using readblocks to do the initial load (which would simplify things if I have two different-sized preloads and/or if I start interleaving across both sides of the disc) is not a significant performance hit



Biggish chunks of data for initial move to $400:
.jmp_buf max 257 bytes but in reality much smaller, not measured/analysed yet


Empirically working out .jmp_buf storage requirement: poking stack page full of $EA and running benchmark then examining stack afterwards, it looks like we never got down below $1E0 (and some of that may be interrupts below our stack, which don't count for .jmp_buf purposes).
Skimming the call code, it doesn't look like Z machine function calls build up state on the 6502 stack - this makes sense, as otherwise a save/restore wouldn't restore that state correctly. So I don't think a sufficiently convoluted program running on Ozmoo can't provoke higher-than-normal 6502 stack use.
